Metadata-Version: 2.4
Name: ann-lab
Version: 0.1.0
Summary: Comprehensive collection of neural network architectures in PyTorch
Author-email: Your Name <your.email@example.com>
License: MIT
Project-URL: Homepage, https://github.com/yourusername/ann-architectures-lab
Project-URL: Documentation, https://github.com/yourusername/ann-architectures-lab
Project-URL: Repository, https://github.com/yourusername/ann-architectures-lab
Project-URL: Issues, https://github.com/yourusername/ann-architectures-lab/issues
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Education
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.11
Description-Content-Type: text/markdown
Requires-Dist: torch>=2.0.0
Requires-Dist: torchvision>=0.15.0
Requires-Dist: numpy>=1.24.0
Requires-Dist: matplotlib>=3.7.0
Provides-Extra: dev
Requires-Dist: pytest>=7.4.0; extra == "dev"
Requires-Dist: pytest-cov>=4.1.0; extra == "dev"
Requires-Dist: black>=23.0.0; extra == "dev"
Requires-Dist: flake8>=6.0.0; extra == "dev"
Requires-Dist: mypy>=1.4.0; extra == "dev"
Provides-Extra: notebooks
Requires-Dist: jupyter>=1.0.0; extra == "notebooks"
Requires-Dist: ipykernel>=6.25.0; extra == "notebooks"
Requires-Dist: ipywidgets>=8.1.0; extra == "notebooks"
Provides-Extra: graph
Requires-Dist: networkx>=3.0; extra == "graph"
Requires-Dist: torch-geometric>=2.3.0; extra == "graph"
Provides-Extra: all
Requires-Dist: pydantic>=2.0.0; extra == "all"
Requires-Dist: networkx>=3.0; extra == "all"
Requires-Dist: torch-geometric>=2.3.0; extra == "all"
Requires-Dist: seaborn>=0.12.0; extra == "all"
Requires-Dist: plotly>=5.16.0; extra == "all"
Requires-Dist: tensorboard>=2.14.0; extra == "all"
Requires-Dist: pandas>=2.0.0; extra == "all"
Requires-Dist: scikit-learn>=1.3.0; extra == "all"

# Artificial Neural Network Architectures Lab

A comprehensive, modular Python implementation of **60+ neural network architectures** built with PyTorch for educational and research purposes.

## ğŸ¯ Overview

This repository provides clean, pedagogical implementations of a wide range of neural network architectures, from classical models to modern deep learning systems. Each implementation prioritizes **clarity, modularity, and educational value** over production optimization.

Perfect for:
- **Students** learning neural network architectures
- **Researchers** prototyping new ideas
- **Engineers** understanding architecture design patterns
- **Educators** teaching deep learning concepts

## ğŸ“¦ Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/ArtificialNeuralNetwork.git
cd ArtificialNeuralNetwork

# Install dependencies
pip install -r requirements.txt

# Install in development mode
pip install -e .
```

## ğŸš€ Quick Start

```python
import torch
from ann_lab.feedforward import MLPClassifier
from ann_lab.conv import SimpleCNN
from ann_lab.recurrent import LSTMClassifier
from ann_lab.transformer import BERTLikeEncoder

# Create a simple MLP
mlp = MLPClassifier(input_dim=784, hidden_dims=[256, 128], num_classes=10)

# Create a CNN for images
cnn = SimpleCNN(in_channels=1, num_classes=10)

# Create an LSTM for sequences
lstm = LSTMClassifier(input_size=100, hidden_size=128, num_classes=10)

# Create a BERT-like encoder
bert = BERTLikeEncoder(vocab_size=30000, embed_dim=768, num_classes=2)
```

Run example training scripts:
```bash
# Train MLP on MNIST
python examples/train_mlp_mnist.py

# Train CNN on MNIST
python examples/train_cnn_mnist.py

# Train LSTM on MNIST (as sequences)
python examples/train_rnn_sequence.py
```

## ğŸ—ï¸ Architecture Families

### 1. **Feedforward Networks** (`ann_lab.feedforward`)
Classic feedforward architectures for tabular and structured data.

| Model | Description | Use Cases |
|-------|-------------|-----------|
| **Perceptron** | Single-layer linear classifier | Binary classification |
| **MLP** | Multi-layer feedforward network | General classification/regression |
| **RBF Network** | Radial basis function network | Function approximation, interpolation |
| **ELM** | Extreme Learning Machine | Fast training, random features |

### 2. **Convolutional Networks** (`ann_lab.conv`)
CNNs for image and spatial data processing.

| Model | Description | Key Features |
|-------|-------------|--------------|
| **SimpleCNN** | Basic CNN | Conv + Pool + FC |
| **LeNet** | Classic digit recognition | Historical architecture |
| **AlexNet** | Deep CNN breakthrough | ReLU, Dropout, Data augmentation |
| **VGGNet** | Very deep network | Small 3x3 filters, uniform design |
| **Inception** | Multi-scale processing | Parallel conv branches |
| **ResNet** | Residual learning | Skip connections, 100+ layers |
| **DenseNet** | Dense connectivity | Feature reuse, efficient |
| **MobileNet** | Lightweight CNN | Depthwise separable convolutions |

### 3. **Recurrent Networks** (`ann_lab.recurrent`)
Sequential models for time series, text, and temporal data.

| Model | Description | Best For |
|-------|-------------|----------|
| **SimpleRNN** | Basic recurrent network | Short sequences |
| **LSTM** | Long Short-Term Memory | Long-term dependencies |
| **GRU** | Gated Recurrent Unit | Faster than LSTM, comparable performance |
| **BiLSTM/BiGRU** | Bidirectional processing | When full sequence is available |
| **Seq2Seq** | Encoder-decoder | Machine translation, summarization |

### 4. **Transformer & Attention** (`ann_lab.transformer`)
Modern attention-based architectures.

| Model | Description | Applications |
|-------|-------------|--------------|
| **MultiHeadAttention** | Core attention mechanism | Building block for transformers |
| **Transformer** | Full encoder-decoder | Translation, seq2seq tasks |
| **BERTLikeEncoder** | Encoder-only | Text classification, NER, QA |
| **GPTLikeDecoder** | Decoder-only | Text generation, language modeling |
| **VisionTransformer** | Transformer for images | Image classification |

### 5. **Autoencoders** (`ann_lab.autoencoder`)
Unsupervised representation learning.

| Model | Description | Use Cases |
|-------|-------------|-----------|
| **BasicAutoencoder** | Standard autoencoder | Dimensionality reduction |
| **ConvAutoencoder** | Convolutional autoencoder | Image compression |
| **DenoisingAutoencoder** | Noise-robust learning | Denoising, robust features |
| **SparseAutoencoder** | Sparsity-constrained | Interpretable features |
| **VAE** | Variational autoencoder | Generative modeling, interpolation |

### 6. **Generative Models** (`ann_lab.generative`)
Models for generating new data samples.

| Model | Description | Generates |
|-------|-------------|-----------|
| **BasicGAN** | Generative Adversarial Network | Synthetic data |
| **DCGAN** | Deep Convolutional GAN | High-quality images |
| **WGAN** | Wasserstein GAN | Stable training, better convergence |
| **SimpleDiffusion** | Denoising diffusion | High-quality samples |
| **CycleGAN** *(scaffold)* | Unpaired image translation | Style transfer |
| **StyleGAN** *(scaffold)* | Style-based generation | Photorealistic faces |

### 7. **Graph Neural Networks** (`ann_lab.graph`)
Learning on graph-structured data.

| Model | Description | Applications |
|-------|-------------|--------------|
| **GNN** | Basic graph neural network | Node/graph classification |
| **GCN** | Graph Convolutional Network | Semi-supervised learning on graphs |
| **GAT** | Graph Attention Network | Attention-based graph learning |
| **GraphSAGE** | Inductive graph learning | Large-scale graphs, unseen nodes |

### 8. **Competitive Networks** (`ann_lab.competitive`)
Unsupervised competitive learning.

| Model | Description | Use Cases |
|-------|-------------|-----------|
| **SOM** | Self-Organizing Map | Visualization, clustering, dimensionality reduction |
| **LVQ** | Learning Vector Quantization | Prototype-based classification |

### 9. **Energy-Based Models** (`ann_lab.energy`)
Energy-based and memory networks.

| Model | Description | Applications |
|-------|-------------|--------------|
| **Hopfield Network** | Associative memory | Pattern retrieval, error correction |
| **RBM** | Restricted Boltzmann Machine | Feature learning, pre-training |
| **DBN** | Deep Belief Network | Unsupervised pre-training |

### 10. **Hybrid Architectures** (`ann_lab.hybrid`)
Specialized and combination architectures.

| Model | Description | Use Cases |
|-------|-------------|-----------|
| **MixtureOfExperts** | Multiple specialized networks | Multi-task learning, diverse data |
| **NTM** *(scaffold)* | Neural Turing Machine | Algorithmic tasks with memory |
| **DNC** *(scaffold)* | Differentiable Neural Computer | Complex reasoning |
| **Neuroevolution** *(toy)* | Evolution-based training | Non-differentiable objectives |
| **SNN** *(scaffold)* | Spiking Neural Network | Neuromorphic computing |

## ğŸ“ Project Structure

```
ArtificialNeuralNetwork/
â”œâ”€â”€ src/ann_lab/              # Main package
â”‚   â”œâ”€â”€ core/                 # Core utilities
â”‚   â”‚   â”œâ”€â”€ base_model.py     # Base model class
â”‚   â”‚   â”œâ”€â”€ datasets.py       # Dataset utilities
â”‚   â”‚   â”œâ”€â”€ training_loops.py # Training/eval loops
â”‚   â”‚   â””â”€â”€ metrics.py        # Evaluation metrics
â”‚   â”œâ”€â”€ feedforward/          # Feedforward networks
â”‚   â”œâ”€â”€ conv/                 # Convolutional networks
â”‚   â”œâ”€â”€ recurrent/            # Recurrent networks
â”‚   â”œâ”€â”€ transformer/          # Transformer models
â”‚   â”œâ”€â”€ autoencoder/          # Autoencoders
â”‚   â”œâ”€â”€ generative/           # Generative models
â”‚   â”œâ”€â”€ graph/                # Graph neural networks
â”‚   â”œâ”€â”€ competitive/          # Competitive learning
â”‚   â”œâ”€â”€ energy/               # Energy-based models
â”‚   â””â”€â”€ hybrid/               # Hybrid architectures
â”œâ”€â”€ examples/                 # Training examples
â”‚   â”œâ”€â”€ train_mlp_mnist.py
â”‚   â”œâ”€â”€ train_cnn_mnist.py
â”‚   â””â”€â”€ train_rnn_sequence.py
â”œâ”€â”€ notebooks/                # Jupyter notebooks (TODO)
â”œâ”€â”€ tests/                    # Unit tests (TODO)
â”œâ”€â”€ requirements.txt          # Dependencies
â”œâ”€â”€ pyproject.toml           # Project configuration
â””â”€â”€ README.md                # This file
```

## ğŸ“ Usage Examples

### Training a Model

```python
import torch
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
from ann_lab.conv import SimpleCNN
from ann_lab.core.training_loops import train_epoch, evaluate

# Setup
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = SimpleCNN(in_channels=1, num_classes=10).to(device)
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# Load data
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])
train_data = datasets.MNIST('./data', train=True, download=True, transform=transform)
train_loader = DataLoader(train_data, batch_size=64, shuffle=True)

# Train
for epoch in range(10):
    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)
    print(f"Epoch {epoch+1}: Loss={train_loss:.4f}, Acc={train_acc:.2f}%")
```

### Using Different Architectures

```python
# Feedforward for tabular data
from ann_lab.feedforward import MLPClassifier
mlp = MLPClassifier(input_dim=100, hidden_dims=[256, 128, 64], num_classes=10)

# LSTM for sequences
from ann_lab.recurrent import LSTMClassifier
lstm = LSTMClassifier(input_size=50, hidden_size=128, num_classes=5)

# Transformer for text
from ann_lab.transformer import BERTLikeEncoder
bert = BERTLikeEncoder(vocab_size=30000, num_classes=2)

# GAN for generation
from ann_lab.generative import DCGAN
gan = DCGAN(latent_dim=100, img_channels=1)

# Graph network
from ann_lab.graph import GCN
gcn = GCN(input_dim=128, hidden_dim=64, output_dim=7)
```

## ğŸ§ª Testing

```bash
# Run all tests (TODO)
pytest tests/

# Run specific test
pytest tests/test_feedforward.py
```

## ğŸ“š Learning Resources

Each architecture file includes:
- **Docstrings** explaining the model and when to use it
- **Comments** on key algorithmic details
- **References** to original papers (where applicable)

## ğŸ¤ Contributing

Contributions are welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Add tests for new architectures
4. Ensure code follows the existing style
5. Submit a pull request

## ğŸ“„ License

MIT License - see LICENSE file for details.

## ğŸ™ Acknowledgments

This project implements architectures from numerous research papers and open-source projects. Key references:
- PyTorch documentation and examples
- Original architecture papers (cited in code)
- Deep Learning textbooks (Goodfellow et al., Dive into Deep Learning)

## ğŸ“ Contact

For questions, suggestions, or issues, please open a GitHub issue.

---

**Note**: This is an educational repository. For production use cases, consider using established frameworks like Hugging Face Transformers, PyTorch Geometric, or timm.
- **WGAN/WGAN-GP** - Wasserstein GAN with gradient penalty
- **CycleGAN** - Unpaired image-to-image translation
- **StyleGAN** - Style-based generator architecture
- **Diffusion Models** - Denoising diffusion probabilistic models

### 7. Graph & Structured Data Models
- **GNN** - Basic Graph Neural Network
- **GCN** - Graph Convolutional Network
- **GAT** - Graph Attention Network
- **GraphSAGE** - Inductive graph learning

### 8. Competitive / Unsupervised Networks
- **Self-Organizing Map (SOM)** - Kohonen networks
- **Learning Vector Quantization (LVQ)** - Prototype-based learning

### 9. Energy-Based & Memory Networks
- **Hopfield Network** - Associative memory network
- **Boltzmann Machine** - Stochastic recurrent network
- **Restricted Boltzmann Machine (RBM)** - Two-layer energy model
- **Deep Belief Network (DBN)** - Stacked RBMs

### 10. Hybrid & Specialized Architectures
- **Neural Turing Machine (NTM)** - Differentiable external memory
- **Differentiable Neural Computer (DNC)** - Advanced memory-augmented network
- **Neuroevolution** - Evolutionary neural architecture search
- **Mixture of Experts (MoE)** - Ensemble of specialized networks
- **Spiking Neural Network (SNN)** - Biologically-inspired neurons

## Installation

### Requirements
- Python 3.11+
- PyTorch 2.0+
- NumPy
- Matplotlib

### Setup

```bash
# Clone the repository
git clone https://github.com/yourusername/ann-architectures-lab.git
cd ann-architectures-lab

# Install dependencies
pip install -r requirements.txt

# Or install in development mode
pip install -e .
```

## Quick Start

### Example: Train an MLP on MNIST

```python
from ann_lab.feedforward.mlp import MLPClassifier
from ann_lab.core.training_loops import train_model
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import torch.optim as optim

# Load MNIST
transform = transforms.Compose([transforms.ToTensor()])
train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

# Create model
model = MLPClassifier(input_dim=784, hidden_dims=[128, 64], num_classes=10)

# Train
optimizer = optim.Adam(model.parameters(), lr=0.001)
train_model(model, train_loader, optimizer, epochs=5)
```

### Run Examples

```bash
# Train MLP on MNIST
python examples/train_mlp_mnist.py

# Train CNN on MNIST
python examples/train_cnn_mnist.py

# Train RNN on sequence data
python examples/train_rnn_sequence.py

# Train transformer on toy task
python examples/train_transformer_toy.py
```

## Project Structure

```
ann-architectures-lab/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ src/
â”‚   â””â”€â”€ ann_lab/
â”‚       â”œâ”€â”€ core/            # Base classes and utilities
â”‚       â”œâ”€â”€ feedforward/     # MLPs and feedforward nets
â”‚       â”œâ”€â”€ conv/            # CNNs and variants
â”‚       â”œâ”€â”€ recurrent/       # RNNs, LSTMs, GRUs
â”‚       â”œâ”€â”€ transformer/     # Attention and transformers
â”‚       â”œâ”€â”€ autoencoder/     # Autoencoders and VAEs
â”‚       â”œâ”€â”€ generative/      # GANs and diffusion models
â”‚       â”œâ”€â”€ graph/           # Graph neural networks
â”‚       â”œâ”€â”€ competitive/     # SOMs and LVQ
â”‚       â”œâ”€â”€ energy/          # Hopfield, RBMs, DBNs
â”‚       â””â”€â”€ hybrid/          # NTMs, MoE, SNNs
â”œâ”€â”€ examples/                # Training scripts
â””â”€â”€ notebooks/               # Jupyter tutorials
```

## Usage Guidelines

- Each architecture is self-contained in its own module
- All models inherit from `BaseModel` for consistency
- Use shared training utilities from `core/training_loops.py`
- Default configurations are small for quick experimentation
- Type hints and docstrings throughout for clarity

## Testing

```bash
# Run all tests
pytest

# Test specific module
pytest tests/test_feedforward.py
```

## Contributing

Contributions are welcome! When adding new architectures:
1. Follow the existing module structure
2. Include clear docstrings explaining the architecture
3. Add a simple example or test
4. Update this README with the new model

## License

MIT License - see LICENSE file for details

## Citation

If you use this repository in your research, please cite:

```bibtex
@software{ann_architectures_lab,
  title = {Artificial Neural Network Architectures Lab},
  author = {Your Name},
  year = {2025},
  url = {https://github.com/yourusername/ann-architectures-lab}
}
```

## Resources

- [PyTorch Documentation](https://pytorch.org/docs/)
- [Deep Learning Book](https://www.deeplearningbook.org/)
- [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/)

---

**Note**: These implementations prioritize pedagogical clarity over production performance. For production use, consider well-tested libraries like `torchvision`, `transformers`, or `torch-geometric`.
